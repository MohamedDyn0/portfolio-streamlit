# content.py

INFO = {
    "name": "Mohamed Dyn",
    "role": "Ing√©nieur Data Science & Data Engineering",
    "school": "INSEA (3√®me ann√©e)",
    "location": "Rabat, Maroc",
    "email": "contact@mohameddyn.me",
    "linkedin": "https://www.linkedin.com/in/mohamed-dyn-301ba6268/",
    "phone": "+212 657 722 751",
    "summary": """
    √âl√®ve-ing√©nieur en derni√®re ann√©e √† l'INSEA, je fusionne Data Engineering, IA G√©n√©rative et Analytics. 
    Je recherche un stage PFE pour transformer des donn√©es complexes en solutions strat√©giques.
    """
}

SKILLS = {
    "Cloud & DevOps": ["AWS (Glue, Lambda, Redshift)", "Azure (Data Factory, Databricks)", "Docker", "CI/CD"],
    "Data Engineering": ["Spark (PySpark)", "Airflow", "Databricks", "DBT", "Kafka", "Hadoop", "DBT", "ETL/ELT"],
    "Data Science & AI": ["LLMs & RAG", "LangChain", "Machine Learning", "TensorFlow", "MLOps"],
    "Languages": ["Python (Avanc√©)", "SQL (Avanc√©)", "Scala", "Java"],
    "BI & Viz": ["Power BI", "Tableau", "AWS QuickSight"]
}

# Dans content.py

EXPERIENCES = [
{
        "role": "Data Scientist (Stagiaire)",
        "company": "TEMACONCEPT",
        "period": "Juil. 2023 - sept. 2023",
        "github": "https://github.com/dynmohamed/Affectation-des-professeurs-CPGE-AI-chatbot",
        "report": "assets/rapport_de_stage_PFA_dyn_mohamed.pdf",
        "desc": "D√©veloppement d'un moteur d'optimisation pour l'affectation des enseignants CPGE et conception d'un syst√®me RAG.",
        
# DESCRIPTION HTML : Narrative, Technique et Fluide
        "description": """
<div style="text-align: justify;">
            Ce projet strat√©gique visait √† refondre int√©gralement le processus d'affectation des enseignants CPGE, jusqu'alors g√©r√© manuellement face √† une <b>complexit√© combinatoire exponentielle</b>. 
            <br><br>
            Mon intervention s'est d'abord port√©e sur la mod√©lisation math√©matique du probl√®me (Linear Assignment Problem). J'ai impl√©ment√© l'<b>algorithme Hongrois (Kuhn-Munkres)</b> pour traiter simultan√©ment des contraintes multidimensionnelles strictes (g√©ographie, sp√©cialit√©, rang p√©dagogique) et garantir une optimalit√© math√©matique impossible √† atteindre humainement.
            <br><br>
            Pour assurer l'adoption de l'outil par les m√©tiers et √©viter l'effet "bo√Æte noire", j'ai con√ßu un module d'<b>IA G√©n√©rative (RAG)</b> connect√© aux r√©sultats. Ce syst√®me permet aux responsables RH d'interroger les affectations en langage naturel (ex: <i>"Pourquoi ce candidat n'a pas eu son v≈ìu n¬∞1 ?"</i>), apportant ainsi une transparence cruciale pour l'aide √† la d√©cision strat√©gique.
         </div>
        """,
        
        # LES CHIFFRES CL√âS RESTENT EN DESSOUS POUR L'IMPACT VISUEL
        "results": [
            "‚ö° <b>Gain de temps massif :</b> Passage d'un traitement de 5 jours √† seulement 2 heures (-95%).",
            "üéØ <b>Optimisation :</b> Maximisation math√©matique de la satisfaction des v≈ìux (100% de couverture).",
            "ü§ñ <b>Innovation :</b> Hybridation r√©ussie entre Recherche Op√©rationnelle (Optimisation) et GenAI (Explicabilit√©)."
        ],
        
        "tags": ["Python", "Recherche Op√©rationnelle", "Algorithme Hongrois", "RAG", "LangChain"]
    },
{
        "role": "Data Analyst (Stagiaire)",
        "company": "Minist√®re charg√© des Relations avec le Parlement",
        "period": "Juil. 2024 - Ao√ªt 2024",
        # Pas de GitHub pour celui-ci, juste le rapport
        "report": "assets/Rapport_du_stage_de_decouverte_DynMohamed.pdf",
        "desc": "Conception de tableaux de bord pour le suivi des activit√©s parlementaires.",
       
        # DESCRIPTION: Focus sur la BI, la Strat√©gie et la Rigueur
        "description": """
        <div style="text-align: justify;">
            Au c≈ìur de la transformation num√©rique de l'administration, ma mission consistait √† moderniser le pilotage des activit√©s parlementaires (Questions √©crites/orales, propositions de loi).
            <br><br>
            J'ai pilot√© la conception d'une <b>architecture d√©cisionnelle (Business Intelligence)</b> compl√®te. Partant de donn√©es brutes et dispers√©es, j'ai r√©alis√© un travail approfondi de <b>Data Cleaning</b> et de mod√©lisation (Star Schema) pour garantir l'int√©grit√© des indicateurs.
            <br><br>
            L'aboutissement a √©t√© le d√©ploiement de tableaux de bord <b>Power BI</b> interactifs. En utilisant des mesures <b>DAX avanc√©es</b>, j'ai fourni aux d√©cideurs une vue en temps r√©el sur les d√©lais de r√©ponse et les th√©matiques l√©gislatives, transformant une gestion administrative en un v√©ritable <b>pilotage strat√©gique par la donn√©e</b>.
        </div>
        """,
        
        # RESULTATS: Impact sur la d√©cision et la qualit√©
        "results": [
            "üìä <b>Gouvernance :</b> Cr√©ation d'une 'Single Source of Truth' fiable pour les statistiques du Minist√®re.",
            "‚ö° <b>Aide √† la d√©cision :</b> Identification imm√©diate des goulots d'√©tranglement (retards de r√©ponse) gr√¢ce aux visuels dynamiques.",
            "üõ†Ô∏è <b>Technique :</b> Mod√©lisation de donn√©es optimis√©e pour des performances de filtrage instantan√©es."
        ],
        
        "tags": ["Power BI", "DAX", "Data Modeling", "SQL", "Gouvernance des donn√©es"]
    },
]

# --- TRES IMPORTANT : AJOUTEZ CECI A LA TOUTE FIN DU FICHIER content.py ---
# Sans cela, vous aurez l'erreur "module content has no attribute DATA"
DATA = {
    "EXPERIENCES": EXPERIENCES,
    "SKILLS": {
        "üíª Langages & BDD": [
            "Python", "SQL", "Java", "C++", "NoSQL", "MongoDB","PostgreSQL"
        ],
        "üß† Data Science & AI": [
            "Machine Learning", "Deep Learning", "TensorFlow", "PyTorch", 
            "Scikit-Learn", "NLP", "Computer Vision", "LLMs (Gemini/GPT)"
        ],
        "‚öôÔ∏è Data Engineering": [
            "Spark", "Hadoop", "Kafka", "Databricks", "DBT", "Airflow", "ETL Pipelines", "Big Data"
        ],
        "‚òÅÔ∏è Cloud & DevOps": [
            "AWS", "Azure", "Docker", "Kubernetes", "Git/GitHub", "CI/CD", "Linux"
        ],
        "üìä Visualisation": [
            "Streamlit", "PowerBI", "Tableau", "Matplotlib", "Seaborn"
        ]
    }
}
# Fusion des projets CV (Business) et GitHub (Tech)
PROJECTS = [
    # --- PROJETS DU CV (Priorit√© Business) ---
    {
        "title": "Clinical Decision Support System (GenAI)",
        "type": "GenAI / MLOps",
        "period": "Projet Personnel Avanc√©",
        # 1. Le Hook : On d√©finit le probl√®me technique tout de suite
        "desc": "Moteur d'inf√©rence m√©dical s√©curis√© (RAG) avec boucle d'alignement expert (RLHF/DPO).",
        
        # 2. Stack de Production (Montre que vous savez d√©ployer, pas juste coder)
        "tech": [
            "Llama-3 (Quantized 4-bit)", # Mod√®le optimis√©
            "LangChain & ChromaDB",      # Orchestration & Vector Store
            "FastAPI (Async)",           # Backend performant
            "Python (Async IO)",         # Pour la performance
            "RLHF (DPO)"                 # Alignement avanc√©
        ],
        
        "link": "https://github.com/dynmohamed/Medical-chatbot-generative-AI",
        "demo_key": "chatbot",

        # 3. L'Histoire Technique (Architecture & Flux de donn√©es)
        "details": """
        <strong>Situation :</strong> L'usage des LLMs standards en milieu m√©dical est critique en raison des "hallucinations" (taux d'erreur ~15-20%) et des risques de fuite de donn√©es patients.
        <br><br>
        <strong>Architecture & Solution :</strong>
        Conception d'un syst√®me <strong>RAG (Retrieval-Augmented Generation)</strong> "Privacy-First" d√©ployable sur site (On-Premise).
        <ul>
            <li><strong>Ingestion Pipeline :</strong> ETL personnalis√© traitant des documents h√©t√©rog√®nes (PDF, Protocoles HTML) avec un <i>Recursive Character Splitter</i> pour pr√©server le contexte s√©mantique.</li>
            <li><strong>Moteur de Recherche Hybride :</strong> Combinaison de recherche vectorielle (Dense Retrieval) et par mots-cl√©s (BM25) via <strong>ChromaDB</strong> pour maximiser le rappel (Recall) des informations m√©dicales.</li>
            <li><strong>Boucle d'Alignement (RLHF) :</strong> Impl√©mentation d'un module de feedback utilisateur collectant les pr√©f√©rences (A/B Testing) pour affiner le mod√®le via <strong>DPO (Direct Preference Optimization)</strong>.</li>
        </ul>
        """,
        
        # 4. Impact Chiffr√© & KPI (Langage Recruteur)
        "impact": """
        üõ°Ô∏è <b>Fiabilit√© Critique :</b> √âlimination des hallucinations via <i>Strict Fact-Grounding</i> (chaque r√©ponse cite sa source).<br>
        ‚ö° <b>Performance :</b> Latence d'inf√©rence r√©duite de 5s √† <b>1.2s</b> gr√¢ce √† la quantification 4-bit du mod√®le Llama-3.<br>
        ‚ö° <b>Accessibilit√© :</b> Transformation d'un mod√®le complexe en une interface web simple utilisable par non-techniciens.        """,

        # 5. Challenges (Preuve de comp√©tence en r√©solution de probl√®mes)
        "challenges": [
            "R√©duction de la latence RAG (Optimisation du Top-K retrieval)",
            "Gestion du 'Lost in the Middle' (Reranking des documents)",
            "Alignement du ton m√©dical (Prompt Engineering avanc√© & Few-Shot)"
        ]
    },
    {
        "title": "Real-time Financial Fraud Detection",
        "type": "FinTech / Cybersec",
        "period": "Projet GitHub",
        "desc": "Syst√®me de d√©tection d'anomalies hybride (Supervis√©/Non-supervis√©) pour s√©curiser les flux financiers en temps r√©el.",
        
        "tech": [
            "Scikit-Learn (Isolation Forest)",
            "XGBoost (Gradient Boosting)",
         
            "SHAP (Explainability)",
            "Pandas (Time-series features)"
        ],
        
        "link": "https://github.com/dynmohamed/detection-de-fraude-en-temps-reel",
        "demo_key": "fraude",

        "details": """
        <strong>Probl√©matique M√©tier :</strong> La d√©tection de fraude est un probl√®me de "d√©s√©quilibre extr√™me" (Imbalanced Classification) : moins de 0.1% des transactions sont frauduleuses. Les mod√®les classiques biaisent vers la classe majoritaire et ratent les fraudes.
        <br><br>
        <strong>Approche Data Science Avanc√©e :</strong>
        <ul>
        <li><strong>1. Feature Engineering Temporel :</strong> Les donn√©es brutes (Montant, Heure) ne suffisent pas. J'ai cr√©√© des agr√©gats temporels complexes (Rolling Windows) : 
            <i>"Moyenne des d√©penses des derni√®res 24h"</i>, <i>"√âcart-type des transactions vs historique client"</i>, <i>"Vitesse g√©ographique entre deux paiements"</i>.</li>
            
        <li><strong>2. Strat√©gie de Mod√©lisation Hybride :</strong>
                <br>- <b>Isolation Forest (Non-supervis√©) :</b> Pour d√©tecter les "Outliers" inconnus (nouveaux modes op√©ratoires de fraudeurs).
                <br>- <b>XGBoost (Supervis√©) :</b> Entra√Æn√© sur les fraudes historiques pour classifier les attaques connues avec une haute pr√©cision.</li>
            
        <li><strong>3. Gestion du D√©s√©quilibre :</strong> Application de la technique <strong>SMOTE</strong> (Synthetic Minority Over-sampling Technique) pendant l'entra√Ænement pour g√©n√©rer des fraudes synth√©tiques et forcer le mod√®le √† apprendre les caract√©ristiques de la classe minoritaire.</li>
        </ul>
        """,
        
        "impact": """
        ‚öñÔ∏è <b>Performance M√©tier :</b> Maximisation du <b>F1-Score</b> plut√¥t que de l'Accuracy, permettant de r√©duire les Faux N√©gatifs (fraudes rat√©es) sans bloquer les clients l√©gitimes (Faux Positifs).<br>
        üîç <b>Explainable AI (XAI) :</b> Int√©gration de la librairie <b>SHAP</b> pour g√©n√©rer une explication locale pour chaque alerte (ex: "Transaction bloqu√©e car montant 5x sup√©rieur √† la moyenne habituelle").<br>
        ‚è±Ô∏è <b>Inf√©rence Temps R√©el :</b> Pipeline optimis√© capable de traiter une transaction et renvoyer un score de risque en millisecondes.
        """,

        "challenges": [
            "Lutte contre le 'Concept Drift' (les sch√©mas de fraude √©voluent dans le temps)",
            "Risque d'Overfitting sur les donn√©es synth√©tiques g√©n√©r√©es par SMOTE",
            "Nettoyage des donn√©es bruit√©es (valeurs manquantes, formats incoh√©rents)"
        ]
    },
    {
        "title": "D√©tection d'Objets (YOLOv8)",
        "type": "Computer Vision",
        "period": "GitHub / Research",
        "desc": "Syst√®me de vision par ordinateur temps r√©el pour la d√©tection d'objets sp√©cifiques (Custom Dataset).",
        
        "tech": [
            "YOLOv8 (Ultralytics)", 
            "OpenCV (Image Processing)", 
            "PyTorch", 
            "Albumentations (Augmentation)", 
            "CUDA (GPU Acceleration)"
        ],
        "link": "https://github.com/dynmohamed/Custom-Object-Detection-using-YOLOv8",
        "demo_key": "yolo",
        
        "details": """
        <strong>D√©fi Technique :</strong> Les mod√®les pr√©-entra√Æn√©s (COCO dataset) √©chouent √† d√©tecter des objets sp√©cifiques ou rares dans des environnements industriels complexes (√©clairage variable, occultations).
        <br><br>
        <strong>Strat√©gie MLOps :</strong>
        <ul>
            <li><strong>Data Curation :</strong> Constitution et annotation manuelle d'un dataset propri√©taire via <b>Roboflow</b>.</li>
            <li><strong>Data Augmentation Avanc√©e :</strong> Application de transformations robustes (Mosa√Øque, Flou gaussien, Cutout) avec la librairie <b>Albumentations</b> pour forcer le mod√®le √† g√©n√©raliser et √©viter l'overfitting.</li>
            <li><strong>Transfer Learning :</strong> Fine-tuning de l'architecture YOLOv8 nano pour atteindre un compromis optimal entre vitesse (FPS) et pr√©cision (mAP).</li>
        </ul>
        """,
        
        "impact": """
        üëÅÔ∏è <b>Pr√©cision :</b> Atteinte d'un <b>mAP@50 de 0.92</b> sur le jeu de test, surpassant les mod√®les g√©n√©riques.<br>
        ‚è±Ô∏è <b>Temps R√©el :</b> Inf√©rence stable √† <b>30+ FPS</b> sur GPU standard, permettant un d√©ploiement sur flux vid√©o live.<br>
        üì¶ <b>D√©ploiement :</b> Export du mod√®le au format ONNX pour une interop√©rabilit√© maximale.
        """,

        "challenges": [
            "Gestion des faux positifs dus aux arri√®re-plans complexes",
            "√âquilibrage des classes (certains objets √©taient sous-repr√©sent√©s)",
            "Optimisation des hyperparam√®tres (Learning Rate, Batch Size) pour la convergence"
        ]
    },
    {
        "title": "Sentiment Analysis",
        "type": "NLP / MLOps",
        "period": "Projet acad√©mique",
        "desc": "API de classification de texte ultra-rapide (<50ms) pour l'analyse de tonalit√© en temps r√©el.",
        
        "tech": [
            "NLTK (Preprocessing)", 
            "Scikit-Learn (TF-IDF & SVM)", 
            "Flask (API Serving)", 
            "Pickle (Serialization)",
            "Python"
        ],
        "link": "https://github.com/dynmohamed/sentiment-prediction",
        "demo_key": "nlp",
        
        "details": """
        <strong>Contexte Engineering :</strong> Les mod√®les de Deep Learning (comme BERT) sont parfois "trop lourds" (overkill) pour des t√¢ches simples n√©cessitant une latence minimale (ex: mod√©ration de chat en direct).
        <br><br>
        <strong>Approche "Lightweight" :</strong>
        <ul>
            <li><strong>NLP Pipeline Classique :</strong> Impl√©mentation d'une cha√Æne de traitement robuste : Nettoyage (Regex) ‚ûî Stopwords removal ‚ûî Stemming (Snowball) ‚ûî Vectorisation (TF-IDF).</li>
            <li><strong>Mod√©lisation Statistique :</strong> Choix d'un algorithme <b>SVM (Support Vector Machine)</b> ou <b>Naive Bayes</b> pour leur efficacit√© redoutable sur des donn√©es textuelles √©parses, offrant un compromis vitesse/pr√©cision id√©al.</li>
            <li><strong>Model Serving :</strong> Encapsulation du mod√®le dans une <b>API REST (Flask)</b>. Le mod√®le est s√©rialis√© (Pickle) au d√©marrage pour garantir des r√©ponses instantan√©es aux requ√™tes HTTP.</li>
        </ul>
        """,
        
        "impact": """
        üöÄ <b>Latence Ultra-Faible :</b> Temps d'inf√©rence sous les <b>20ms</b>, permettant une int√©gration synchrone dans des applications web.<br>
        üîß <b>Int√©grabilit√© :</b> Architecture micro-service d√©coupl√©e, facile √† consommer par n'importe quel front-end.<br>
        üìâ <b>Efficacit√© :</b> Consommation m√©moire minime (<200MB RAM) compar√©e aux mod√®les Transformers (>2GB).
        """,

        "challenges": [
            "Gestion des n√©gations (ex: 'Pas mal' doit √™tre positif)",
            "R√©duction de la dimensionnalit√© du vocabulaire (Feature Selection)",
            "Nettoyage des caract√®res sp√©ciaux et Emojis"
        ]
    },

    {
        "title": "Architecture ELT Serverless (AWS)",
        "type": "Data Engineering / Cloud",
        "period": "Mars 2025",
        "desc": "Pipeline 'Event-Driven' enti√®rement automatis√© sur AWS (Zero-Infrastructure).",
        
        "tech": [
            "AWS Glue (Spark Jobs)", 
            "AWS Lambda (Triggers)", 
            "Amazon S3 (Data Lake)", 
            "Redshift (Data Warehousing)", 
            "IAC (Terraform)"
        ],
        
        "details": """
        <strong>D√©fi Infrastructure :</strong> L'entreprise traitait ses logs via des scripts manuels, causant une latence de 24h et des co√ªts de maintenance √©lev√©s (serveurs EC2 inactifs 80% du temps).
        <br><br>
        <strong>Solution Serverless :</strong> Conception d'une architecture r√©active :
        <ul>
            <li><strong>Ingestion Event-Driven :</strong> Utilisation de <b>S3 Event Notifications</b> pour d√©clencher une fonction <b>AWS Lambda</b> d√®s l'arriv√©e d'un fichier, √©liminant le besoin d'orchestrateur externe pour les t√¢ches simples.</li>
            <li><strong>Transformation Scalable :</strong> Jobs <b>AWS Glue</b> (PySpark) pour nettoyer, d√©dupliquer et convertir les logs bruts (JSON) en format colonnaire optimis√© (Parquet).</li>
            <li><strong>Warehousing :</strong> Chargement automatique dans <b>Redshift</b> via la commande <i>COPY</i> pour permettre des requ√™tes analytiques SQL performantes.</li>
        </ul>
        """,
        
        "impact": """
        ‚ö° <b>Latence R√©duite :</b> Passage d'un batch quotidien √† un traitement en <b>quasi temps r√©el (15 min)</b>.<br>
        üí∞ <b>FinOps :</b> R√©duction de 40% de la facture Cloud gr√¢ce au mod√®le "Pay-as-you-go" (Serverless) vs serveurs d√©di√©s.<br>
        üìà <b>Scalabilit√© :</b> Le pipeline encaisse automatiquement les pics de charge sans intervention humaine.
        """,

        "challenges": [
            "Gestion des 'Cold Starts' des fonctions Lambda",
            "Optimisation du partitionnement S3 (Hive-style) pour r√©duire les co√ªts de scan",
            "Impl√©mentation de Dead Letter Queues (DLQ) pour rejouer les √©v√©nements √©chou√©s"
        ]
    },
    {
        "title": "Flux ETL Big Data (Azure & Databricks)",
        "type": "Big Data Engineering",
        "period": "Avr. 2025",
        "desc": "Impl√©mentation d'une 'Medallion Architecture' (Bronze/Silver/Gold) pour un Data Lake d'entreprise.",
        
        "tech": [
            "Azure Data Factory (Orchestration)", 
            "Databricks (PySpark)", 
            "ADLS Gen2 (Storage)", 
            "Delta Lake (ACID)", 
            "Power BI"
        ],
        
        "details": """
        <strong>Probl√©matique Data Quality :</strong> Les donn√©es sources (ERP, CRM) √©taient cloisonn√©es et incoh√©rentes, rendant impossible la cr√©ation d'une "Single Source of Truth".
        <br><br>
        <strong>Architecture Medallion :</strong>
        <ul>
            <li><strong>Couche Bronze (Raw) :</strong> Ingestion brute des donn√©es historiques via <b>Azure Data Factory</b> dans le Data Lake (ADLS Gen2).</li>
            <li><strong>Couche Silver (Clean) :</strong> Nettoyage et standardisation via <b>Spark sur Databricks</b>. Utilisation du format <b>Delta Lake</b> pour garantir les transactions ACID et permettre le "Time Travel" (versioning).</li>
            <li><strong>Couche Gold (Aggregated) :</strong> Cr√©ation de tables dimensionnelles (Star Schema) pr√™tes pour la consommation BI.</li>
        </ul>
        """,
        
        "impact": """
        ‚úÖ <b>Qualit√© de Donn√©e :</b> Fiabilit√© des rapports pass√©e de 60% √† 99.9% gr√¢ce aux contraintes de sch√©ma (Schema Enforcement) de Delta Lake.<br>
        üöÄ <b>Performance :</b> Traitement des donn√©es 40% plus rapide gr√¢ce √† l'optimisation du moteur Photon de Databricks.<br>
        üîÑ <b>Auditabilit√© :</b> Capacit√© de revenir √† n'importe quelle version ant√©rieure de la donn√©e via les logs de transaction.
        """,

        "challenges": [
            "Gestion des 'Schema Drifts' (√©volution de la structure des donn√©es sources)",
            "Optimisation des Shuffle Partitions dans Spark pour g√©rer les gros volumes (TB)",
            "S√©curisation des acc√®s via Azure Key Vault et Service Principals"
        ]
    },
    {
        "title": "Speech Emotion Recognition (SER)",
        "type": "Deep Learning / Audio",
        "period": "GitHub",
        "desc": "Classification des √©motions humaines √† partir de signaux audio bruts via CNN.",
        
        "tech": [
            "Librosa (Feature Extraction)", 
            "TensorFlow / Keras", 
            "CNN (Convolutional Neural Network)", 
            "Matplotlib (Spectrograms)"
        ],
        "link": "https://github.com/dynmohamed/Speech-Emotion-Recognition---Sound-Classification",
        
        "details": """
        <strong>Complexit√© du Signal :</strong> L'audio brut est une donn√©e non structur√©e difficile √† exploiter directement par des algorithmes classiques. L'enjeu est d'extraire des caract√©ristiques repr√©sentatives de l'√©tat √©motionnel (Col√®re, Joie, Tristesse).
        <br><br>
        <strong>Pipeline de Traitement :</strong>
        <ul>
            <li><strong>Feature Extraction :</strong> Transformation des fichiers audio (.wav) en repr√©sentations visuelles et spectrales : extraction des <b>MFCCs</b> (Mel-Frequency Cepstral Coefficients) et des <b>Mel-Spectrograms</b> qui capturent la texture du son.</li>
            <li><strong>Architecture Deep Learning :</strong> Conception d'un r√©seau de neurones convolutif (CNN) adapt√© aux donn√©es s√©quentielles, capable d'apprendre des motifs temporels et fr√©quentiels dans les spectrogrammes.</li>
        </ul>
        """,
        
        "impact": """
        üéß <b>Analyse Acoustique :</b> Capacit√© √† distinguer 7 √©motions fondamentales ind√©pendamment du contenu s√©mantique (ce qui est dit importe peu, c'est le "ton" qui compte).<br>
        üìä <b>Performance :</b> Accuracy de 80%+ sur le dataset de r√©f√©rence (RAVDESS/TESS).<br>
        üß© <b>Applications :</b> Brique technologique utilisable pour l'analyse de satisfaction en centre d'appels.
        """,

        "challenges": [
            "Normalisation des dur√©es audio (Padding/Truncating)",
            "R√©duction du bruit de fond dans les enregistrements",
            "Augmentation des donn√©es audio (Time stretching, Pitch shifting) pour enrichir le dataset"
        ]
    },   
{
        "title": "Market Intelligence Bancaire",
        "type": "Web Scraping / Analytics",
        "period": "GitHub",
        "desc": "Pipeline d'extraction et d'analyse de sentiment sur les avis clients des banques marocaines.",
        
        "tech": [
            "Selenium & BeautifulSoup", 
            "NLP (Sentiment Analysis)", 
            "Plotly / Dash", 
            "Pandas"
        ],
        "link": "https://github.com/dynmohamed/Analyzing-Customer-Reviews-of-Bank-Agencies-in-Morocco-using-a-Modern-Data-Stack",
        
        "details": """
        <strong>Besoin Business :</strong> Les banques manquent de visibilit√© consolid√©e sur la satisfaction client exprim√©e publiquement (Google Maps, Trustpilot), perdant des opportunit√©s d'am√©lioration.
        <br><br>
        <strong>Solution End-to-End :</strong>
        <ul>
            <li><strong>Scraping Robuste :</strong> D√©veloppement de bots <b>Selenium</b> capables de naviguer, scroller et extraire des milliers d'avis tout en g√©rant les d√©lais et les erreurs de chargement.</li>
            <li><strong>Enrichissement NLP :</strong> Analyse de chaque commentaire pour extraire le sentiment global (Polarit√©) et les sujets r√©currents (ex: "Attente", "Application mobile").</li>
            <li><strong>Visualisation :</strong> Restitution des insights via des dashboards interactifs pour le benchmarking concurrentiel.</li>
        </ul>
        """,
        
        "impact": """
        üí° <b>Insights Strat√©giques :</b> Identification automatique des agences les moins performantes.<br>
        üìä <b>Benchmarking :</b> Comparaison quantitative de la satisfaction (NPS estim√©) entre les acteurs du march√©.<br>
        üîÑ <b>Automatisation :</b> Remplacement d'une veille manuelle fastidieuse par un script ex√©cutable √† la demande.
        """,

        "challenges": [
            "Maintenance des s√©lecteurs CSS face aux mises √† jour des sites web",
            "Nettoyage des donn√©es textuelles tr√®s bruit√©es (m√©lange Fran√ßais / Darija / Arabe)",
            "D√©tection et filtrage des faux avis (Spam)"
        ]
    },
    {
        "title": "Hate Speech Detection (NLP)",
        "type": "NLP / Trust & Safety",
        "period": "GitHub",
        "desc": "Syst√®me de mod√©ration automatique bas√© sur les Transformers (BERT) pour identifier les contenus toxiques.",
        
        "tech": [
            "Hugging Face Transformers", 
            "BERT (Fine-tuning)", 
            "PyTorch", 
            "Scikit-Learn (Evaluation)",
            "Pandas"
        ],
        "link": "https://github.com/dynmohamed/Hate-Speach-Detection",
        
        "details": """
        <strong>Enjeu Soci√©tal :</strong> Les m√©thodes bas√©es sur des mots-cl√©s (Regex) sont inefficaces face au sarcasme, aux fautes d'orthographe ou au contexte implicite des discours haineux.
        <br><br>
        <strong>Approche State-of-the-Art :</strong>
        <ul>
            <li><strong>Preprocessing NLP :</strong> Nettoyage avanc√© (Tokenization, Lemmatization, suppression URLs) pour r√©duire le bruit textuel.</li>
            <li><strong>Transfer Learning :</strong> Utilisation d'un mod√®le <b>BERT</b> (Bidirectional Encoder Representations from Transformers) pr√©-entra√Æn√©. Contrairement aux mod√®les simples (LSTM), BERT comprend le contexte bidirectionnel de la phrase.</li>
            <li><strong>Fine-Tuning :</strong> R√©-entra√Ænement des derni√®res couches du mod√®le sur un corpus classifi√© pour sp√©cialiser la d√©tection.</li>
        </ul>
        """,
        
        "impact": """
        üõ°Ô∏è <b>S√©curit√© :</b> Automatisation de la mod√©ration avec un Recall √©lev√© (minimisation des contenus haineux non d√©tect√©s).<br>
        üß† <b>Contextualisation :</b> Distinction efficace entre une discussion offensive et l'usage de termes argotiques non haineux.<br>
        """,

        "challenges": [
            "Gestion du d√©s√©quilibre des classes (peu de contenu haineux vs contenu normal)",
            "Optimisation du temps d'entra√Ænement sur GPU (Gradient Accumulation)",
            "Traitement du langage informel et des abr√©viations SMS"
        ]
    }
]
# --- AJOUTER √Ä LA FIN DE content.py ---

DEMOS = [
    {
        "key": "yolo",
        "title": "üëÅÔ∏è Vision",
        "desc": "D√©tection d'objets (YOLOv8).",
        "btn_label": "Lancer üöÄ"
    },
    {
        "key": "nlp",
        "title": "üß† NLP",
        "desc": "Analyse de Sentiment.",
        "btn_label": "Lancer üìù"
    },
    {
        "key": "chatbot",
        "title": "ü©∫ Sant√©",
        "desc": "Assistant M√©dical (RAG).",
        "btn_label": "Lancer ü§ñ"
    },
    {
        "key": "fraude",
        "title": "üí≥ FinTech",
        "desc": "D√©tection de Fraude (ML).",
        "btn_label": "Lancer üîç"
    }
]

# ==============================================================================
# 2. AJOUT DU CONTENU ANGLAIS (NOUVEAU)
# ==============================================================================

INFO_EN = {
    "name": "Mohamed Dyn",
    "role": "Data Science & Data Engineering Engineer",
    "location": "Rabat, Morocco",
    "summary": """
    Final-year engineering student at INSEA, I bridge the gap between Data Engineering, Generative AI, and Analytics.
    I am looking for an End-of-Studies Internship (PFE) to transform complex data into strategic solutions.
    """
}

EXPERIENCES_EN = [
    {
        "role": "Data Scientist (Intern)",
        "company": "TEMACONCEPT",
        "period": "Jul 2023 - Sep 2023",
        "github": "https://github.com/dynmohamed/Affectation-des-professeurs-CPGE-AI-chatbot",
        "report": "assets/rapport_de_stage_PFA_dyn_mohamed.pdf",
        "desc": "Development of an optimization engine for CPGE teacher assignments and design of a RAG system.",
        
        # HTML DESCRIPTION: Narrative, Technical, and Fluid
        "description": """
        <div style="text-align: justify;">
            This strategic project aimed to completely overhaul the CPGE teacher assignment process, previously managed manually in the face of <b>exponential combinatorial complexity</b>. 
            <br><br>
            My intervention first focused on the mathematical modeling of the problem (Linear Assignment Problem). I implemented the <b>Hungarian algorithm (Kuhn-Munkres)</b> to simultaneously handle strict multidimensional constraints (geography, specialty, pedagogical rank) and guarantee mathematical optimality impossible to achieve manually.
            <br><br>
            To ensure tool adoption by stakeholders and avoid the "black box" effect, I designed a <b>Generative AI (RAG)</b> module connected to the results. This system allows HR managers to query assignments in natural language (e.g., <i>"Why didn't this candidate get their #1 choice?"</i>), thus providing crucial transparency for strategic decision-making.
        </div>
        """,
        
        # KEY FIGURES REMAIN BELOW FOR VISUAL IMPACT
        "results": [
            "‚ö° <b>Massive time saving:</b> Reduced processing time from 5 days to just 2 hours (-95%).",
            "üéØ <b>Optimization:</b> Mathematical maximization of preference satisfaction (100% coverage).",
            "ü§ñ <b>Innovation:</b> Successful hybridization between Operations Research (Optimization) and GenAI (Explainability)."
        ],
        
        "tags": ["Python", "Operations Research", "Hungarian Algorithm", "RAG", "LangChain"]
    },
    {
        "role": "Data Analyst (Intern)",
        "company": "Ministry of Parliamentary Relations",
        "period": "Jul 2024 - Aug 2024",
        # No GitHub for this one, just the report
        "report": "assets/Rapport_du_stage_de_decouverte_DynMohamed.pdf",
        "desc": "Design of dashboards for monitoring parliamentary activities.",
        
        # DESCRIPTION: Focus on BI, Strategy, and Rigor
        "description": """
        <div style="text-align: justify;">
            At the heart of the administration's digital transformation, my mission was to modernize the management of parliamentary activities (Written/oral questions, bill proposals).
            <br><br>
            I led the design of a complete <b>Business Intelligence (BI) architecture</b>. Starting from raw and scattered data, I performed in-depth <b>Data Cleaning</b> and modeling (Star Schema) to ensure indicator integrity.
            <br><br>
            The result was the deployment of interactive <b>Power BI</b> dashboards. By using advanced <b>DAX measures</b>, I provided decision-makers with a real-time view of response times and legislative themes, transforming administrative management into true <b>data-driven strategic steering</b>.
        </div>
        """,
        
        # RESULTS: Impact on decision and quality
        "results": [
            "üìä <b>Governance:</b> Creation of a reliable 'Single Source of Truth' for Ministry statistics.",
            "‚ö° <b>Decision Support:</b> Immediate identification of bottlenecks (response delays) thanks to dynamic visuals.",
            "üõ†Ô∏è <b>Technical:</b> Optimized data modeling for instant filtering performance."
        ],
        
        "tags": ["Power BI", "DAX", "Data Modeling", "SQL", "Data Governance"]
    },
]
# Merging CV (Business) and GitHub (Tech) projects
PROJECTS_EN = [
    # --- CV PROJECTS (Business Priority) ---
    {
        "title": "Clinical Decision Support System (GenAI)",
        "type": "GenAI / MLOps",
        "period": "Advanced Personal Project",
        # 1. The Hook: Define the technical problem immediately
        "desc": "Secure medical inference engine (RAG) with expert alignment loop (RLHF/DPO).",
        
        # 2. Production Stack (Shows deployment skills, not just coding)
        "tech": [
            "Llama-3 (Quantized 4-bit)", # Optimized model
            "LangChain & ChromaDB",      # Orchestration & Vector Store
            "FastAPI (Async)",           # High-performance Backend
            "Python (Async IO)",         # For concurrency
            "RLHF (DPO)"                 # Advanced Alignment
        ],
        
        "link": "https://github.com/dynmohamed/Medical-chatbot-generative-AI",
        "demo_key": "chatbot",

        # 3. The Technical Story (Architecture & Data Flow)
        "details": """
        <strong>Situation:</strong> The use of standard LLMs in medical settings is critical due to "hallucinations" (error rate ~15-20%) and patient data privacy risks.
        <br><br>
        <strong>Architecture & Solution:</strong>
        Design of a <strong>RAG (Retrieval-Augmented Generation)</strong> "Privacy-First" system deployable On-Premise.
        <ul>
            <li><strong>Ingestion Pipeline:</strong> Custom ETL processing heterogeneous documents (PDF, HTML Protocols) with a <i>Recursive Character Splitter</i> to preserve semantic context.</li>
            <li><strong>Hybrid Search Engine:</strong> Combination of vector search (Dense Retrieval) and keyword search (BM25) via <strong>ChromaDB</strong> to maximize medical information Recall.</li>
            <li><strong>Alignment Loop (RLHF):</strong> Implementation of a user feedback module collecting preferences (A/B Testing) to refine the model via <strong>DPO (Direct Preference Optimization)</strong>.</li>
        </ul>
        """,
        
        # 4. Impact & KPIs (Recruiter Language)
        "impact": """
        üõ°Ô∏è <b>Critical Reliability:</b> Elimination of hallucinations via <i>Strict Fact-Grounding</i> (every answer cites its source).<br>
        ‚ö° <b>Performance:</b> Inference latency reduced from 5s to <b>1.2s</b> thanks to Llama-3 4-bit quantization.<br>
        ‚ö° <b>Accessibility:</b> Transformation of a complex model into a simple web interface usable by non-technical staff.
        """,

        # 5. Challenges (Proof of problem-solving skills)
        "challenges": [
            "Reducing RAG latency (Top-K retrieval optimization)",
            "Handling 'Lost in the Middle' phenomenon (Document Reranking)",
            "Aligning medical tone (Advanced Prompt Engineering & Few-Shot)"
        ]
    },
    {
        "title": "Real-time Financial Fraud Detection",
        "type": "FinTech / Cybersec",
        "period": "GitHub Project",
        "desc": "Hybrid anomaly detection system (Supervised/Unsupervised) to secure financial flows in real-time.",
        
        "tech": [
            "Scikit-Learn (Isolation Forest)",
            "XGBoost (Gradient Boosting)",
            "SHAP (Explainability)",
            "Pandas (Time-series features)"
        ],
        
        "link": "https://github.com/dynmohamed/detection-de-fraude-en-temps-reel",
        "demo_key": "fraude",

        "details": """
        <strong>Business Problem:</strong> Fraud detection is an "extreme imbalance" problem (Imbalanced Classification): less than 0.1% of transactions are fraudulent. Classic models bias towards the majority class and miss frauds.
        <br><br>
        <strong>Advanced Data Science Approach:</strong>
        <ul>
        <li><strong>1. Temporal Feature Engineering:</strong> Raw data (Amount, Time) is insufficient. I created complex temporal aggregates (Rolling Windows): 
            <i>"Average spend over last 24h"</i>, <i>"Standard deviation vs client history"</i>, <i>"Geographic velocity between two payments"</i>.</li>
            
        <li><strong>2. Hybrid Modeling Strategy:</strong>
                <br>- <b>Isolation Forest (Unsupervised):</b> To detect unknown "Outliers" (new fraud patterns).
                <br>- <b>XGBoost (Supervised):</b> Trained on historical fraud to classify known attacks with high precision.</li>
            
        <li><strong>3. Handling Imbalance:</strong> Application of <strong>SMOTE</strong> (Synthetic Minority Over-sampling Technique) during training to generate synthetic frauds and force the model to learn minority class characteristics.</li>
        </ul>
        """,
        
        "impact": """
        ‚öñÔ∏è <b>Business Performance:</b> Maximization of <b>F1-Score</b> rather than Accuracy, reducing False Negatives (missed fraud) without blocking legitimate clients (False Positives).<br>
        üîç <b>Explainable AI (XAI):</b> Integration of <b>SHAP</b> library to generate local explanations for each alert (e.g., "Transaction blocked because amount is 5x higher than usual average").<br>
        ‚è±Ô∏è <b>Real-Time Inference:</b> Optimized pipeline capable of processing a transaction and returning a risk score in milliseconds.
        """,

        "challenges": [
            "Combating 'Concept Drift' (fraud patterns evolve over time)",
            "Risk of Overfitting on synthetic data generated by SMOTE",
            "Cleaning noisy data (missing values, inconsistent formats)"
        ]
    },
    {
        "title": "Object Detection (YOLOv8)",
        "type": "Computer Vision",
        "period": "GitHub / Research",
        "desc": "Real-time computer vision system for specific object detection (Custom Dataset).",
        
        "tech": [
            "YOLOv8 (Ultralytics)", 
            "OpenCV (Image Processing)", 
            "PyTorch", 
            "Albumentations (Augmentation)", 
            "CUDA (GPU Acceleration)"
        ],
        "link": "https://github.com/dynmohamed/Custom-Object-Detection-using-YOLOv8",
        "demo_key": "yolo",
        
        "details": """
        <strong>Technical Challenge:</strong> Pre-trained models (COCO dataset) fail to detect specific or rare objects in complex industrial environments (variable lighting, occlusions).
        <br><br>
        <strong>MLOps Strategy:</strong>
        <ul>
            <li><strong>Data Curation:</strong> Creation and manual annotation of a proprietary dataset via <b>Roboflow</b>.</li>
            <li><strong>Advanced Data Augmentation:</strong> Application of robust transformations (Mosaic, Gaussian Blur, Cutout) with the <b>Albumentations</b> library to force generalization and avoid overfitting.</li>
            <li><strong>Transfer Learning:</strong> Fine-tuning of the YOLOv8 nano architecture to reach an optimal compromise between speed (FPS) and precision (mAP).</li>
        </ul>
        """,
        
        "impact": """
        üëÅÔ∏è <b>Precision:</b> Achieved a <b>mAP@50 of 0.92</b> on the test set, outperforming generic models.<br>
        ‚è±Ô∏è <b>Real-Time:</b> Stable inference at <b>30+ FPS</b> on standard GPU, allowing deployment on live video streams.<br>
        üì¶ <b>Deployment:</b> Model export to ONNX format for maximum interoperability.
        """,

        "challenges": [
            "Handling false positives due to complex backgrounds",
            "Class balancing (some objects were under-represented)",
            "Hyperparameter optimization (Learning Rate, Batch Size) for convergence"
        ]
    },
    {
        "title": "Sentiment Analysis",
        "type": "NLP / MLOps",
        "period": "Academic Project",
        "desc": "Ultra-fast text classification API (<50ms) for real-time tone analysis.",
        
        "tech": [
            "NLTK (Preprocessing)", 
            "Scikit-Learn (TF-IDF & SVM)", 
            "Flask (API Serving)", 
            "Pickle (Serialization)",
            "Python"
        ],
        "link": "https://github.com/dynmohamed/sentiment-prediction",
        "demo_key": "nlp",
        
        "details": """
        <strong>Engineering Context:</strong> Deep Learning models (like BERT) are sometimes "overkill" for simple tasks requiring minimal latency (e.g., live chat moderation).
        <br><br>
        <strong>"Lightweight" Approach:</strong>
        <ul>
            <li><strong>Classic NLP Pipeline:</strong> Implementation of a robust processing chain: Cleaning (Regex) ‚ûî Stopwords removal ‚ûî Stemming (Snowball) ‚ûî Vectorization (TF-IDF).</li>
            <li><strong>Statistical Modeling:</strong> Choice of <b>SVM (Support Vector Machine)</b> or <b>Naive Bayes</b> algorithm for their formidable efficiency on sparse text data, offering an ideal speed/precision trade-off.</li>
            <li><strong>Model Serving:</strong> Encapsulation of the model in a <b>REST API (Flask)</b>. The model is serialized (Pickle) at startup to guarantee instant responses to HTTP requests.</li>
        </ul>
        """,
        
        "impact": """
        üöÄ <b>Ultra-Low Latency:</b> Inference time under <b>20ms</b>, allowing synchronous integration into web applications.<br>
        üîß <b>Integrability:</b> Decoupled micro-service architecture, easy to consume by any front-end.<br>
        üìâ <b>Efficiency:</b> Minimal memory consumption (<200MB RAM) compared to Transformer models (>2GB).
        """,

        "challenges": [
            "Handling negations (e.g., 'Not bad' should be positive)",
            "Dimensionality reduction of vocabulary (Feature Selection)",
            "Cleaning special characters and Emojis"
        ]
    },

    {
        "title": "Serverless ELT Architecture (AWS)",
        "type": "Data Engineering / Cloud",
        "period": "Mar. 2025",
        "desc": "Fully automated 'Event-Driven' pipeline on AWS (Zero-Infrastructure).",
        
        "tech": [
            "AWS Glue (Spark Jobs)", 
            "AWS Lambda (Triggers)", 
            "Amazon S3 (Data Lake)", 
            "Redshift (Data Warehousing)", 
            "IAC (Terraform)"
        ],
        
        "details": """
        <strong>Infrastructure Challenge:</strong> The company processed logs via manual scripts, causing 24h latency and high maintenance costs (EC2 servers inactive 80% of the time).
        <br><br>
        <strong>Serverless Solution:</strong> Design of a reactive architecture:
        <ul>
            <li><strong>Event-Driven Ingestion:</strong> Using <b>S3 Event Notifications</b> to trigger an <b>AWS Lambda</b> function as soon as a file arrives, eliminating the need for an external orchestrator for simple tasks.</li>
            <li><strong>Scalable Transformation:</strong> <b>AWS Glue</b> (PySpark) jobs to clean, deduplicate, and convert raw logs (JSON) into optimized columnar format (Parquet).</li>
            <li><strong>Warehousing:</strong> Automatic loading into <b>Redshift</b> via the <i>COPY</i> command to enable high-performance SQL analytical queries.</li>
        </ul>
        """,
        
        "impact": """
        ‚ö° <b>Reduced Latency:</b> Shift from daily batch to <b>near real-time (15 min)</b> processing.<br>
        üí∞ <b>FinOps:</b> 40% reduction in Cloud bill thanks to "Pay-as-you-go" (Serverless) model vs dedicated servers.<br>
        üìà <b>Scalability:</b> The pipeline automatically handles load spikes without human intervention.
        """,

        "challenges": [
            "Managing 'Cold Starts' of Lambda functions",
            "Optimizing S3 partitioning (Hive-style) to reduce scan costs",
            "Implementing Dead Letter Queues (DLQ) to replay failed events"
        ]
    },
    {
        "title": "Big Data ETL Pipeline (Azure & Databricks)",
        "type": "Big Data Engineering",
        "period": "Apr. 2025",
        "desc": "Implementation of a 'Medallion Architecture' (Bronze/Silver/Gold) for an Enterprise Data Lake.",
        
        "tech": [
            "Azure Data Factory (Orchestration)", 
            "Databricks (PySpark)", 
            "ADLS Gen2 (Storage)", 
            "Delta Lake (ACID)", 
            "Power BI"
        ],
        
        "details": """
        <strong>Data Quality Problem:</strong> Source data (ERP, CRM) was siloed and inconsistent, making it impossible to create a "Single Source of Truth".
        <br><br>
        <strong>Medallion Architecture:</strong>
        <ul>
            <li><strong>Bronze Layer (Raw):</strong> Raw ingestion of historical data via <b>Azure Data Factory</b> into the Data Lake (ADLS Gen2).</li>
            <li><strong>Silver Layer (Clean):</strong> Cleaning and standardization via <b>Spark on Databricks</b>. Usage of <b>Delta Lake</b> format to guarantee ACID transactions and enable "Time Travel" (versioning).</li>
            <li><strong>Gold Layer (Aggregated):</strong> Creation of dimensional tables (Star Schema) ready for BI consumption.</li>
        </ul>
        """,
        
        "impact": """
        ‚úÖ <b>Data Quality:</b> Report reliability increased from 60% to 99.9% thanks to Delta Lake Schema Enforcement constraints.<br>
        üöÄ <b>Performance:</b> Data processing 40% faster thanks to Databricks Photon engine optimization.<br>
        üîÑ <b>Auditability:</b> Ability to revert to any previous data version via transaction logs.
        """,

        "challenges": [
            "Managing 'Schema Drifts' (evolution of source data structure)",
            "Optimizing Spark Shuffle Partitions to handle large volumes (TB)",
            "Securing access via Azure Key Vault and Service Principals"
        ]
    },
    {
        "title": "Speech Emotion Recognition (SER)",
        "type": "Deep Learning / Audio",
        "period": "GitHub",
        "desc": "Classification of human emotions from raw audio signals via CNN.",
        
        "tech": [
            "Librosa (Feature Extraction)", 
            "TensorFlow / Keras", 
            "CNN (Convolutional Neural Network)", 
            "Matplotlib (Spectrograms)"
        ],
        "link": "https://github.com/dynmohamed/Speech-Emotion-Recognition---Sound-Classification",
        
        "details": """
        <strong>Signal Complexity:</strong> Raw audio is unstructured data difficult to exploit directly by classic algorithms. The challenge is to extract features representative of the emotional state (Anger, Joy, Sadness).
        <br><br>
        <strong>Processing Pipeline:</strong>
        <ul>
            <li><strong>Feature Extraction:</strong> Transforming audio files (.wav) into visual and spectral representations: extraction of <b>MFCCs</b> (Mel-Frequency Cepstral Coefficients) and <b>Mel-Spectrograms</b> which capture sound texture.</li>
            <li><strong>Deep Learning Architecture:</strong> Design of a Convolutional Neural Network (CNN) adapted to sequential data, capable of learning temporal and frequency patterns in spectrograms.</li>
        </ul>
        """,
        
        "impact": """
        üéß <b>Acoustic Analysis:</b> Ability to distinguish 7 fundamental emotions independently of semantic content (what is said matters little, it's the "tone" that counts).<br>
        üìä <b>Performance:</b> 80%+ Accuracy on the reference dataset (RAVDESS/TESS).<br>
        üß© <b>Applications:</b> Tech brick usable for satisfaction analysis in call centers.
        """,

        "challenges": [
            "Normalizing audio durations (Padding/Truncating)",
            "Reducing background noise in recordings",
            "Audio Data Augmentation (Time stretching, Pitch shifting) to enrich dataset"
        ]
    },   
    {
        "title": "Banking Market Intelligence",
        "type": "Web Scraping / Analytics",
        "period": "GitHub",
        "desc": "Pipeline for extraction and sentiment analysis of Moroccan bank customer reviews.",
        
        "tech": [
            "Selenium & BeautifulSoup", 
            "NLP (Sentiment Analysis)", 
            "Plotly / Dash", 
            "Pandas"
        ],
        "link": "https://github.com/dynmohamed/Analyzing-Customer-Reviews-of-Bank-Agencies-in-Morocco-using-a-Modern-Data-Stack",
        
        "details": """
        <strong>Business Need:</strong> Banks lack consolidated visibility on publicly expressed customer satisfaction (Google Maps, Trustpilot), missing improvement opportunities.
        <br><br>
        <strong>End-to-End Solution:</strong>
        <ul>
            <li><strong>Robust Scraping:</strong> Development of <b>Selenium</b> bots capable of navigating, scrolling, and extracting thousands of reviews while handling timeouts and loading errors.</li>
            <li><strong>NLP Enrichment:</strong> Analysis of each comment to extract global sentiment (Polarity) and recurring topics (e.g., "Waiting time", "Mobile App").</li>
            <li><strong>Visualization:</strong> Insight delivery via interactive dashboards for competitive benchmarking.</li>
        </ul>
        """,
        
        "impact": """
        üí° <b>Strategic Insights:</b> Automatic identification of underperforming branches.<br>
        üìä <b>Benchmarking:</b> Quantitative comparison of satisfaction (estimated NPS) between market players.<br>
        üîÑ <b>Automation:</b> Replacement of tedious manual monitoring with an on-demand executable script.
        """,

        "challenges": [
            "Maintaining CSS selectors against website updates",
            "Cleaning highly noisy text data (mix of French / Darija / Arabic)",
            "Detection and filtering of fake reviews (Spam)"
        ]
    },
    {
        "title": "Hate Speech Detection (NLP)",
        "type": "NLP / Trust & Safety",
        "period": "GitHub",
        "desc": "Automated moderation system based on Transformers (BERT) to identify toxic content.",
        
        "tech": [
            "Hugging Face Transformers", 
            "BERT (Fine-tuning)", 
            "PyTorch", 
            "Scikit-Learn (Evaluation)",
            "Pandas"
        ],
        "link": "https://github.com/dynmohamed/Hate-Speach-Detection",
        
        "details": """
        <strong>Societal Issue:</strong> Keyword-based methods (Regex) are ineffective against sarcasm, spelling mistakes, or implicit context of hate speech.
        <br><br>
        <strong>State-of-the-Art Approach:</strong>
        <ul>
            <li><strong>NLP Preprocessing:</strong> Advanced cleaning (Tokenization, Lemmatization, URL removal) to reduce text noise.</li>
            <li><strong>Transfer Learning:</strong> Use of a pre-trained <b>BERT</b> (Bidirectional Encoder Representations from Transformers) model. Unlike simple models (LSTM), BERT understands the bidirectional context of the sentence.</li>
            <li><strong>Fine-Tuning:</strong> Retraining the model's final layers on a classified corpus to specialize detection.</li>
        </ul>
        """,
        
        "impact": """
        üõ°Ô∏è <b>Safety:</b> Automated moderation with high Recall (minimizing undetected hate content).<br>
        üß† <b>Contextualization:</b> Effective distinction between offensive discussion and the use of non-hateful slang terms.<br>
        """,

        "challenges": [
            "Handling class imbalance (few hate content vs. normal content)",
            "Optimizing training time on GPU (Gradient Accumulation)",
            "Processing informal language and SMS abbreviations"
        ]
    }
]